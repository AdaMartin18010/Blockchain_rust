# å­˜å‚¨ç³»ç»Ÿè®¾è®¡

## ğŸ“‹ ç›®å½•

- [1. å­˜å‚¨æ¶æ„æ¦‚è§ˆ](#1-å­˜å‚¨æ¶æ„æ¦‚è§ˆ)
  - [1.1 å­˜å‚¨å±‚æ¬¡ç»“æ„](#11-å­˜å‚¨å±‚æ¬¡ç»“æ„)
  - [1.2 å­˜å‚¨ç±»å‹](#12-å­˜å‚¨ç±»å‹)
  - [1.3 æ•°æ®ç»„ç»‡](#13-æ•°æ®ç»„ç»‡)
- [2. åŒºå—å­˜å‚¨](#2-åŒºå—å­˜å‚¨)
  - [2.1 åŒºå—æ•°æ®ç»“æ„](#21-åŒºå—æ•°æ®ç»“æ„)
  - [2.2 åŒºå—ç´¢å¼•](#22-åŒºå—ç´¢å¼•)
  - [2.3 åŒºå—æ£€ç´¢](#23-åŒºå—æ£€ç´¢)
- [3. çŠ¶æ€å­˜å‚¨](#3-çŠ¶æ€å­˜å‚¨)
  - [3.1 Merkle Patricia Trie](#31-merkle-patricia-trie)
  - [3.2 çŠ¶æ€å¿«ç…§](#32-çŠ¶æ€å¿«ç…§)
  - [3.3 çŠ¶æ€å‰ªæ](#33-çŠ¶æ€å‰ªæ)
- [4. äº¤æ˜“å­˜å‚¨](#4-äº¤æ˜“å­˜å‚¨)
  - [4.1 äº¤æ˜“æ± è®¾è®¡](#41-äº¤æ˜“æ± è®¾è®¡)
  - [4.2 äº¤æ˜“ç´¢å¼•](#42-äº¤æ˜“ç´¢å¼•)
  - [4.3 å†å²äº¤æ˜“æŸ¥è¯¢](#43-å†å²äº¤æ˜“æŸ¥è¯¢)
- [5. å­˜å‚¨å¼•æ“](#5-å­˜å‚¨å¼•æ“)
  - [5.1 RocksDBé›†æˆ](#51-rocksdbé›†æˆ)
  - [5.2 LMDBé›†æˆ](#52-lmdbé›†æˆ)
  - [5.3 è‡ªå®šä¹‰å­˜å‚¨å¼•æ“](#53-è‡ªå®šä¹‰å­˜å‚¨å¼•æ“)
- [6. ç¼“å­˜ç­–ç•¥](#6-ç¼“å­˜ç­–ç•¥)
  - [6.1 LRUç¼“å­˜](#61-lruç¼“å­˜)
  - [6.2 å¤šçº§ç¼“å­˜](#62-å¤šçº§ç¼“å­˜)
  - [6.3 é¢„è¯»ä¼˜åŒ–](#63-é¢„è¯»ä¼˜åŒ–)
- [7. æ•°æ®æŒä¹…åŒ–](#7-æ•°æ®æŒä¹…åŒ–)
  - [7.1 WALæ—¥å¿—](#71-walæ—¥å¿—)
  - [7.2 æ£€æŸ¥ç‚¹](#72-æ£€æŸ¥ç‚¹)
  - [7.3 å´©æºƒæ¢å¤](#73-å´©æºƒæ¢å¤)
- [8. å­˜å‚¨ä¼˜åŒ–](#8-å­˜å‚¨ä¼˜åŒ–)
  - [8.1 å‹ç¼©ç­–ç•¥](#81-å‹ç¼©ç­–ç•¥)
  - [8.2 æ•°æ®åˆ†ç‰‡](#82-æ•°æ®åˆ†ç‰‡)
  - [8.3 å½’æ¡£ä¸å‰ªæ](#83-å½’æ¡£ä¸å‰ªæ)

## 1. å­˜å‚¨æ¶æ„æ¦‚è§ˆ

### 1.1 å­˜å‚¨å±‚æ¬¡ç»“æ„

```rust
/// å­˜å‚¨å±‚æ¬¡ç»“æ„
#[derive(Debug)]
pub struct StorageArchitecture {
    /// L1: å†…å­˜ç¼“å­˜å±‚
    cache_layer: Arc<CacheLayer>,
    /// L2: æŒä¹…åŒ–å­˜å‚¨å±‚
    storage_layer: Arc<StorageLayer>,
    /// L3: å½’æ¡£å­˜å‚¨å±‚
    archive_layer: Option<Arc<ArchiveLayer>>,
}

/// ç¼“å­˜å±‚ï¼ˆå†…å­˜ï¼‰
#[derive(Debug)]
pub struct CacheLayer {
    /// åŒºå—ç¼“å­˜
    block_cache: Arc<RwLock<LruCache<Hash, Block>>>,
    /// çŠ¶æ€ç¼“å­˜
    state_cache: Arc<RwLock<LruCache<Hash, StateNode>>>,
    /// äº¤æ˜“ç¼“å­˜
    tx_cache: Arc<RwLock<LruCache<Hash, Transaction>>>,
}

/// æŒä¹…åŒ–å­˜å‚¨å±‚
#[derive(Debug)]
pub struct StorageLayer {
    /// åŒºå—å­˜å‚¨
    block_store: Arc<BlockStore>,
    /// çŠ¶æ€å­˜å‚¨
    state_store: Arc<StateStore>,
    /// äº¤æ˜“å­˜å‚¨
    tx_store: Arc<TransactionStore>,
}

/// å½’æ¡£å­˜å‚¨å±‚
#[derive(Debug)]
pub struct ArchiveLayer {
    /// å†å²åŒºå—å½’æ¡£
    archive_store: Arc<ArchiveStore>,
}

impl StorageArchitecture {
    /// è¯»å–åŒºå—ï¼ˆå¤šå±‚æŸ¥æ‰¾ï¼‰
    pub async fn get_block(&self, hash: &Hash) -> Result<Option<Block>, Error> {
        // 1. å°è¯•ä»ç¼“å­˜è¯»å–
        if let Some(block) = self.cache_layer.get_block(hash).await? {
            return Ok(Some(block));
        }
        
        // 2. ä»æŒä¹…åŒ–å±‚è¯»å–
        if let Some(block) = self.storage_layer.get_block(hash).await? {
            // å†™å…¥ç¼“å­˜
            self.cache_layer.put_block(hash.clone(), block.clone()).await?;
            return Ok(Some(block));
        }
        
        // 3. ä»å½’æ¡£å±‚è¯»å–
        if let Some(archive) = &self.archive_layer {
            if let Some(block) = archive.get_block(hash).await? {
                return Ok(Some(block));
            }
        }
        
        Ok(None)
    }
    
    /// å†™å…¥åŒºå—
    pub async fn put_block(&self, hash: Hash, block: Block) -> Result<(), Error> {
        // 1. å†™å…¥ç¼“å­˜
        self.cache_layer.put_block(hash.clone(), block.clone()).await?;
        
        // 2. æŒä¹…åŒ–åˆ°å­˜å‚¨å±‚
        self.storage_layer.put_block(hash, block).await?;
        
        Ok(())
    }
}
```

### 1.2 å­˜å‚¨ç±»å‹

```rust
/// å­˜å‚¨ç±»å‹
#[derive(Debug, Clone)]
pub enum StorageType {
    /// å…¨èŠ‚ç‚¹å­˜å‚¨ï¼šå­˜å‚¨æ‰€æœ‰å†å²æ•°æ®
    FullNode {
        /// æ˜¯å¦å¯ç”¨å½’æ¡£æ¨¡å¼
        archive: bool,
        /// æ˜¯å¦å¯ç”¨å‰ªæ
        pruning: bool,
    },
    /// è½»èŠ‚ç‚¹å­˜å‚¨ï¼šåªå­˜å‚¨åŒºå—å¤´
    LightNode {
        /// åŒºå—å¤´æ•°é‡é™åˆ¶
        header_limit: usize,
    },
    /// éªŒè¯èŠ‚ç‚¹å­˜å‚¨ï¼šå­˜å‚¨æœ€è¿‘Nä¸ªåŒºå—çš„çŠ¶æ€
    ValidatorNode {
        /// çŠ¶æ€å†å²æ·±åº¦
        state_depth: u64,
    },
    /// å½’æ¡£èŠ‚ç‚¹å­˜å‚¨ï¼šå­˜å‚¨æ‰€æœ‰å†å²çŠ¶æ€
    ArchiveNode,
}

/// å­˜å‚¨é…ç½®
#[derive(Debug, Clone)]
pub struct StorageConfig {
    /// å­˜å‚¨ç±»å‹
    pub storage_type: StorageType,
    /// æ•°æ®ç›®å½•
    pub data_dir: PathBuf,
    /// ç¼“å­˜å¤§å°
    pub cache_size: usize,
    /// æ˜¯å¦å¯ç”¨å‹ç¼©
    pub compression: bool,
    /// æ˜¯å¦å¯ç”¨æ ¡éªŒå’Œ
    pub checksum: bool,
}
```

### 1.3 æ•°æ®ç»„ç»‡

```rust
/// æ•°æ®ç»„ç»‡ç»“æ„
pub struct DataOrganization {
    /// åŒºå—æ•°æ®ï¼šæŒ‰é«˜åº¦ç»„ç»‡
    /// data/blocks/000000/000001.blk
    blocks_by_height: PathBuf,
    
    /// åŒºå—ç´¢å¼•ï¼šå“ˆå¸Œ -> æ–‡ä»¶ä½ç½®
    /// data/indexes/block_hash.idx
    block_index: PathBuf,
    
    /// çŠ¶æ€æ•°æ®ï¼šMerkleæ ‘èŠ‚ç‚¹
    /// data/state/nodes/
    state_nodes: PathBuf,
    
    /// äº¤æ˜“æ•°æ®ï¼šæŒ‰å“ˆå¸Œç»„ç»‡
    /// data/transactions/
    transactions: PathBuf,
    
    /// å…ƒæ•°æ®
    /// data/metadata/
    metadata: PathBuf,
}

impl DataOrganization {
    /// è·å–åŒºå—æ–‡ä»¶è·¯å¾„
    pub fn block_file_path(&self, height: u64) -> PathBuf {
        let folder = format!("{:06}", height / 1000);
        let filename = format!("{:06}.blk", height % 1000);
        self.blocks_by_height.join(folder).join(filename)
    }
    
    /// è·å–åŒºå—ç´¢å¼•è·¯å¾„
    pub fn block_index_path(&self) -> PathBuf {
        self.block_index.join("block_hash.idx")
    }
}
```

## 2. åŒºå—å­˜å‚¨

### 2.1 åŒºå—æ•°æ®ç»“æ„

```rust
use serde::{Serialize, Deserialize};

/// åŒºå—å­˜å‚¨é¡¹
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StoredBlock {
    /// åŒºå—æ•°æ®
    pub block: Block,
    /// åŒºå—é«˜åº¦
    pub height: u64,
    /// åŒºå—å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    pub size: u64,
    /// å­˜å‚¨æ—¶é—´æˆ³
    pub stored_at: SystemTime,
    /// ç¡®è®¤æ•°
    pub confirmations: u64,
}

/// åŒºå—å…ƒæ•°æ®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BlockMetadata {
    /// åŒºå—å“ˆå¸Œ
    pub hash: Hash,
    /// åŒºå—é«˜åº¦
    pub height: u64,
    /// æ–‡ä»¶åç§»é‡
    pub file_offset: u64,
    /// åŒºå—å¤§å°
    pub size: u32,
    /// äº¤æ˜“æ•°é‡
    pub tx_count: u32,
}
```

### 2.2 åŒºå—ç´¢å¼•

```rust
use rocksdb::{DB, Options};

/// åŒºå—å­˜å‚¨å®ç°
#[derive(Debug)]
pub struct BlockStore {
    /// RocksDBå®ä¾‹
    db: Arc<DB>,
    /// åŒºå—æ•°æ®ç›®å½•
    data_dir: PathBuf,
    /// åŒºå—ç´¢å¼•
    index: Arc<RwLock<HashMap<Hash, BlockMetadata>>>,
}

impl BlockStore {
    /// åˆ›å»ºåŒºå—å­˜å‚¨
    pub fn new(data_dir: PathBuf) -> Result<Self, Error> {
        let db_path = data_dir.join("blocks_db");
        let mut opts = Options::default();
        opts.create_if_missing(true);
        opts.set_compression_type(rocksdb::DBCompressionType::Lz4);
        
        let db = DB::open(&opts, db_path)?;
        
        Ok(Self {
            db: Arc::new(db),
            data_dir,
            index: Arc::new(RwLock::new(HashMap::new())),
        })
    }
    
    /// å­˜å‚¨åŒºå—
    pub async fn put_block(&self, hash: Hash, block: Block) -> Result<(), Error> {
        let height = block.header.height;
        
        // 1. åºåˆ—åŒ–åŒºå—
        let stored_block = StoredBlock {
            block: block.clone(),
            height,
            size: 0, // è®¡ç®—å®é™…å¤§å°
            stored_at: SystemTime::now(),
            confirmations: 0,
        };
        let data = bincode::serialize(&stored_block)?;
        
        // 2. å†™å…¥RocksDB
        self.db.put(hash.as_bytes(), &data)?;
        
        // 3. æ›´æ–°ç´¢å¼•
        let metadata = BlockMetadata {
            hash: hash.clone(),
            height,
            file_offset: 0,
            size: data.len() as u32,
            tx_count: block.transactions.len() as u32,
        };
        self.index.write().await.insert(hash.clone(), metadata);
        
        // 4. åˆ›å»ºé«˜åº¦ç´¢å¼•
        self.db.put(
            format!("height:{}", height).as_bytes(),
            hash.as_bytes()
        )?;
        
        Ok(())
    }
    
    /// è¯»å–åŒºå—
    pub async fn get_block(&self, hash: &Hash) -> Result<Option<Block>, Error> {
        match self.db.get(hash.as_bytes())? {
            Some(data) => {
                let stored_block: StoredBlock = bincode::deserialize(&data)?;
                Ok(Some(stored_block.block))
            },
            None => Ok(None),
        }
    }
    
    /// æ ¹æ®é«˜åº¦è·å–åŒºå—
    pub async fn get_block_by_height(&self, height: u64) -> Result<Option<Block>, Error> {
        // 1. ä»é«˜åº¦ç´¢å¼•è·å–å“ˆå¸Œ
        let key = format!("height:{}", height);
        match self.db.get(key.as_bytes())? {
            Some(hash_bytes) => {
                let hash = Hash::from_bytes(&hash_bytes)?;
                self.get_block(&hash).await
            },
            None => Ok(None),
        }
    }
    
    /// è·å–åŒºå—å…ƒæ•°æ®
    pub async fn get_block_metadata(&self, hash: &Hash) -> Result<Option<BlockMetadata>, Error> {
        Ok(self.index.read().await.get(hash).cloned())
    }
    
    /// æ‰¹é‡è¯»å–åŒºå—
    pub async fn get_blocks(&self, hashes: &[Hash]) -> Result<Vec<Block>, Error> {
        let mut blocks = Vec::new();
        
        for hash in hashes {
            if let Some(block) = self.get_block(hash).await? {
                blocks.push(block);
            }
        }
        
        Ok(blocks)
    }
}
```

### 2.3 åŒºå—æ£€ç´¢

```rust
/// åŒºå—æŸ¥è¯¢æ¥å£
impl BlockStore {
    /// è·å–æœ€æ–°åŒºå—
    pub async fn get_latest_block(&self) -> Result<Option<Block>, Error> {
        // ä»å…ƒæ•°æ®ä¸­è·å–æœ€æ–°é«˜åº¦
        let latest_height_key = b"latest_height";
        match self.db.get(latest_height_key)? {
            Some(height_bytes) => {
                let height = u64::from_be_bytes(height_bytes.as_slice().try_into()?);
                self.get_block_by_height(height).await
            },
            None => Ok(None),
        }
    }
    
    /// è·å–åŒºå—èŒƒå›´
    pub async fn get_block_range(
        &self,
        start_height: u64,
        end_height: u64
    ) -> Result<Vec<Block>, Error> {
        let mut blocks = Vec::new();
        
        for height in start_height..=end_height {
            if let Some(block) = self.get_block_by_height(height).await? {
                blocks.push(block);
            }
        }
        
        Ok(blocks)
    }
    
    /// æŸ¥è¯¢åŒ…å«æŒ‡å®šäº¤æ˜“çš„åŒºå—
    pub async fn find_block_by_transaction(&self, tx_hash: &Hash) -> Result<Option<Block>, Error> {
        // ä»äº¤æ˜“ç´¢å¼•æŸ¥æ‰¾
        let key = format!("tx:{}", tx_hash);
        match self.db.get(key.as_bytes())? {
            Some(block_hash_bytes) => {
                let block_hash = Hash::from_bytes(&block_hash_bytes)?;
                self.get_block(&block_hash).await
            },
            None => Ok(None),
        }
    }
}
```

## 3. çŠ¶æ€å­˜å‚¨

### 3.1 Merkle Patricia Trie

```rust
/// Merkle Patricia Trie èŠ‚ç‚¹
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum MPTNode {
    /// å¶å­èŠ‚ç‚¹
    Leaf {
        key: Vec<u8>,
        value: Vec<u8>,
    },
    /// æ‰©å±•èŠ‚ç‚¹
    Extension {
        prefix: Vec<u8>,
        child: Hash,
    },
    /// åˆ†æ”¯èŠ‚ç‚¹ï¼ˆ16ä¸ªå­èŠ‚ç‚¹ + å¯é€‰å€¼ï¼‰
    Branch {
        children: [Option<Hash>; 16],
        value: Option<Vec<u8>>,
    },
    /// ç©ºèŠ‚ç‚¹
    Empty,
}

/// Merkle Patricia Trie å®ç°
#[derive(Debug)]
pub struct MerklePatriciaTrie {
    /// å­˜å‚¨å¼•æ“
    db: Arc<DB>,
    /// æ ¹å“ˆå¸Œ
    root: Arc<RwLock<Hash>>,
}

impl MerklePatriciaTrie {
    /// æ’å…¥é”®å€¼å¯¹
    pub async fn insert(&self, key: &[u8], value: Vec<u8>) -> Result<(), Error> {
        let root = *self.root.read().await;
        let new_root = self.insert_node(root, key, value, 0).await?;
        *self.root.write().await = new_root;
        Ok(())
    }
    
    /// é€’å½’æ’å…¥èŠ‚ç‚¹
    async fn insert_node(
        &self,
        node_hash: Hash,
        key: &[u8],
        value: Vec<u8>,
        depth: usize
    ) -> Result<Hash, Error> {
        // è¯»å–èŠ‚ç‚¹
        let node = self.get_node(&node_hash).await?;
        
        match node {
            MPTNode::Empty => {
                // åˆ›å»ºæ–°çš„å¶å­èŠ‚ç‚¹
                let leaf = MPTNode::Leaf {
                    key: key[depth..].to_vec(),
                    value,
                };
                self.put_node(&leaf).await
            },
            
            MPTNode::Leaf { key: leaf_key, value: leaf_value } => {
                // æ‰¾åˆ°å…±åŒå‰ç¼€
                let common_prefix_len = Self::common_prefix_length(
                    &key[depth..],
                    &leaf_key
                );
                
                if common_prefix_len == leaf_key.len() && depth + common_prefix_len == key.len() {
                    // é”®å®Œå…¨åŒ¹é…ï¼Œæ›´æ–°å€¼
                    let new_leaf = MPTNode::Leaf {
                        key: leaf_key,
                        value,
                    };
                    self.put_node(&new_leaf).await
                } else {
                    // éœ€è¦åˆ†è£‚èŠ‚ç‚¹
                    self.split_leaf(
                        &leaf_key,
                        leaf_value,
                        &key[depth..],
                        value,
                        common_prefix_len
                    ).await
                }
            },
            
            MPTNode::Extension { prefix, child } => {
                let common_prefix_len = Self::common_prefix_length(
                    &key[depth..],
                    &prefix
                );
                
                if common_prefix_len == prefix.len() {
                    // æ²¿ç€æ‰©å±•èŠ‚ç‚¹ç»§ç»­
                    let new_child = self.insert_node(
                        child,
                        key,
                        value,
                        depth + prefix.len()
                    ).await?;
                    
                    let ext = MPTNode::Extension {
                        prefix,
                        child: new_child,
                    };
                    self.put_node(&ext).await
                } else {
                    // åˆ†è£‚æ‰©å±•èŠ‚ç‚¹
                    self.split_extension(
                        &prefix,
                        child,
                        &key[depth..],
                        value,
                        common_prefix_len
                    ).await
                }
            },
            
            MPTNode::Branch { mut children, value: branch_value } => {
                if depth == key.len() {
                    // åœ¨åˆ†æ”¯èŠ‚ç‚¹è®¾ç½®å€¼
                    let branch = MPTNode::Branch {
                        children,
                        value: Some(value),
                    };
                    self.put_node(&branch).await
                } else {
                    // ç»§ç»­æ²¿ç€åˆ†æ”¯
                    let nibble = key[depth] as usize;
                    let child_hash = children[nibble].unwrap_or(Hash::empty());
                    let new_child = self.insert_node(
                        child_hash,
                        key,
                        value,
                        depth + 1
                    ).await?;
                    
                    children[nibble] = Some(new_child);
                    let branch = MPTNode::Branch {
                        children,
                        value: branch_value,
                    };
                    self.put_node(&branch).await
                }
            },
        }
    }
    
    /// æŸ¥è¯¢é”®å€¼
    pub async fn get(&self, key: &[u8]) -> Result<Option<Vec<u8>>, Error> {
        let root = *self.root.read().await;
        self.get_node_value(root, key, 0).await
    }
    
    /// é€’å½’æŸ¥è¯¢èŠ‚ç‚¹å€¼
    async fn get_node_value(
        &self,
        node_hash: Hash,
        key: &[u8],
        depth: usize
    ) -> Result<Option<Vec<u8>>, Error> {
        let node = self.get_node(&node_hash).await?;
        
        match node {
            MPTNode::Empty => Ok(None),
            
            MPTNode::Leaf { key: leaf_key, value } => {
                if &key[depth..] == leaf_key.as_slice() {
                    Ok(Some(value))
                } else {
                    Ok(None)
                }
            },
            
            MPTNode::Extension { prefix, child } => {
                if key[depth..].starts_with(&prefix) {
                    self.get_node_value(child, key, depth + prefix.len()).await
                } else {
                    Ok(None)
                }
            },
            
            MPTNode::Branch { children, value } => {
                if depth == key.len() {
                    Ok(value)
                } else {
                    let nibble = key[depth] as usize;
                    if let Some(child_hash) = children[nibble] {
                        self.get_node_value(child_hash, key, depth + 1).await
                    } else {
                        Ok(None)
                    }
                }
            },
        }
    }
    
    /// è®¡ç®—å…±åŒå‰ç¼€é•¿åº¦
    fn common_prefix_length(a: &[u8], b: &[u8]) -> usize {
        a.iter().zip(b.iter()).take_while(|(x, y)| x == y).count()
    }
    
    /// åˆ†è£‚å¶å­èŠ‚ç‚¹
    async fn split_leaf(
        &self,
        old_key: &[u8],
        old_value: Vec<u8>,
        new_key: &[u8],
        new_value: Vec<u8>,
        common_prefix_len: usize
    ) -> Result<Hash, Error> {
        // å®ç°å¶å­èŠ‚ç‚¹åˆ†è£‚é€»è¾‘
        todo!()
    }
    
    /// åˆ†è£‚æ‰©å±•èŠ‚ç‚¹
    async fn split_extension(
        &self,
        prefix: &[u8],
        child: Hash,
        new_key: &[u8],
        new_value: Vec<u8>,
        common_prefix_len: usize
    ) -> Result<Hash, Error> {
        // å®ç°æ‰©å±•èŠ‚ç‚¹åˆ†è£‚é€»è¾‘
        todo!()
    }
    
    /// å­˜å‚¨èŠ‚ç‚¹
    async fn put_node(&self, node: &MPTNode) -> Result<Hash, Error> {
        let data = bincode::serialize(node)?;
        let hash = Hash::hash(&data);
        self.db.put(hash.as_bytes(), &data)?;
        Ok(hash)
    }
    
    /// è¯»å–èŠ‚ç‚¹
    async fn get_node(&self, hash: &Hash) -> Result<MPTNode, Error> {
        match self.db.get(hash.as_bytes())? {
            Some(data) => {
                let node: MPTNode = bincode::deserialize(&data)?;
                Ok(node)
            },
            None => Ok(MPTNode::Empty),
        }
    }
    
    /// è·å–æ ¹å“ˆå¸Œ
    pub async fn root_hash(&self) -> Hash {
        *self.root.read().await
    }
}
```

### 3.2 çŠ¶æ€å¿«ç…§

```rust
/// çŠ¶æ€å¿«ç…§ç®¡ç†å™¨
#[derive(Debug)]
pub struct StateSnapshot {
    /// å¿«ç…§ID
    id: SnapshotId,
    /// åŒºå—é«˜åº¦
    block_height: u64,
    /// çŠ¶æ€æ ¹å“ˆå¸Œ
    state_root: Hash,
    /// å¿«ç…§æ—¶é—´
    timestamp: SystemTime,
    /// å¿«ç…§å­˜å‚¨
    storage: Arc<DB>,
}

impl StateSnapshot {
    /// åˆ›å»ºå¿«ç…§
    pub async fn create(
        block_height: u64,
        state_root: Hash,
        mpt: &MerklePatriciaTrie
    ) -> Result<Self, Error> {
        let id = SnapshotId::new();
        let snapshot_path = format!("snapshots/snapshot_{}", id);
        
        let mut opts = Options::default();
        opts.create_if_missing(true);
        let storage = Arc::new(DB::open(&opts, &snapshot_path)?);
        
        // éå†å¹¶å¤åˆ¶æ‰€æœ‰çŠ¶æ€
        // å®ç°çŠ¶æ€å¤åˆ¶é€»è¾‘
        
        Ok(Self {
            id,
            block_height,
            state_root,
            timestamp: SystemTime::now(),
            storage,
        })
    }
    
    /// ä»å¿«ç…§æ¢å¤
    pub async fn restore(&self, mpt: &MerklePatriciaTrie) -> Result<(), Error> {
        // ä»å¿«ç…§æ¢å¤çŠ¶æ€æ ‘
        Ok(())
    }
}

#[derive(Debug, Clone, Hash, Eq, PartialEq)]
pub struct SnapshotId(uuid::Uuid);

impl SnapshotId {
    pub fn new() -> Self {
        Self(uuid::Uuid::new_v4())
    }
}
```

### 3.3 çŠ¶æ€å‰ªæ

```rust
/// çŠ¶æ€å‰ªæç®¡ç†å™¨
#[derive(Debug)]
pub struct StatePruning {
    /// ä¿ç•™æ·±åº¦ï¼ˆä¿ç•™æœ€è¿‘Nä¸ªåŒºå—çš„çŠ¶æ€ï¼‰
    retention_depth: u64,
    /// å½“å‰æœ€æ–°é«˜åº¦
    latest_height: Arc<RwLock<u64>>,
    /// çŠ¶æ€å­˜å‚¨
    state_store: Arc<StateStore>,
}

impl StatePruning {
    /// æ‰§è¡Œå‰ªæ
    pub async fn prune(&self) -> Result<PruningStats, Error> {
        let latest = *self.latest_height.read().await;
        
        if latest < self.retention_depth {
            return Ok(PruningStats::default());
        }
        
        let prune_before = latest - self.retention_depth;
        
        let mut stats = PruningStats::default();
        
        // æ ‡è®°å¯ä»¥åˆ é™¤çš„çŠ¶æ€èŠ‚ç‚¹
        for height in 0..prune_before {
            let pruned = self.prune_height(height).await?;
            stats.blocks_pruned += 1;
            stats.nodes_removed += pruned;
        }
        
        Ok(stats)
    }
    
    /// å‰ªææŒ‡å®šé«˜åº¦çš„çŠ¶æ€
    async fn prune_height(&self, height: u64) -> Result<usize, Error> {
        // å®ç°çŠ¶æ€å‰ªæé€»è¾‘
        // 1. è·å–è¯¥é«˜åº¦çš„çŠ¶æ€æ ¹
        // 2. éå†çŠ¶æ€æ ‘ï¼Œæ ‡è®°ä¸å†è¢«å¼•ç”¨çš„èŠ‚ç‚¹
        // 3. åˆ é™¤æ ‡è®°çš„èŠ‚ç‚¹
        Ok(0)
    }
}

#[derive(Debug, Default)]
pub struct PruningStats {
    pub blocks_pruned: u64,
    pub nodes_removed: usize,
    pub space_freed: u64,
}
```

## 4. äº¤æ˜“å­˜å‚¨

### 4.1 äº¤æ˜“æ± è®¾è®¡

```rust
use std::collections::BTreeMap;

/// äº¤æ˜“æ± 
#[derive(Debug)]
pub struct Mempool {
    /// å¾…å¤„ç†äº¤æ˜“ï¼ˆæŒ‰nonceæ’åºï¼‰
    pending: Arc<RwLock<BTreeMap<Address, BTreeMap<u64, Transaction>>>>,
    /// é˜Ÿåˆ—äº¤æ˜“
    queued: Arc<RwLock<HashMap<Hash, Transaction>>>,
    /// äº¤æ˜“ç´¢å¼•
    by_hash: Arc<RwLock<HashMap<Hash, Transaction>>>,
    /// é…ç½®
    config: MempoolConfig,
}

#[derive(Debug, Clone)]
pub struct MempoolConfig {
    /// æœ€å¤§äº¤æ˜“æ•°
    pub max_transactions: usize,
    /// æœ€å°Gasä»·æ ¼
    pub min_gas_price: u64,
    /// äº¤æ˜“è¿‡æœŸæ—¶é—´
    pub tx_ttl: Duration,
}

impl Mempool {
    /// æ·»åŠ äº¤æ˜“
    pub async fn add_transaction(&self, tx: Transaction) -> Result<(), Error> {
        let tx_hash = tx.hash();
        
        // 1. éªŒè¯äº¤æ˜“
        self.validate_transaction(&tx)?;
        
        // 2. æ£€æŸ¥äº¤æ˜“æ± æ˜¯å¦å·²æ»¡
        if self.is_full().await {
            self.evict_lowest_priority_tx().await?;
        }
        
        // 3. æ·»åŠ åˆ°äº¤æ˜“æ± 
        {
            let mut by_hash = self.by_hash.write().await;
            by_hash.insert(tx_hash.clone(), tx.clone());
        }
        
        // 4. æ ¹æ®è´¦æˆ·çŠ¶æ€åˆ†ç±»
        let sender = tx.sender();
        let nonce = tx.nonce();
        
        let mut pending = self.pending.write().await;
        pending.entry(sender)
            .or_insert_with(BTreeMap::new)
            .insert(nonce, tx);
        
        Ok(())
    }
    
    /// ç§»é™¤äº¤æ˜“
    pub async fn remove_transaction(&self, tx_hash: &Hash) -> Result<(), Error> {
        let mut by_hash = self.by_hash.write().await;
        
        if let Some(tx) = by_hash.remove(tx_hash) {
            let sender = tx.sender();
            let nonce = tx.nonce();
            
            let mut pending = self.pending.write().await;
            if let Some(sender_txs) = pending.get_mut(&sender) {
                sender_txs.remove(&nonce);
                if sender_txs.is_empty() {
                    pending.remove(&sender);
                }
            }
        }
        
        Ok(())
    }
    
    /// è·å–å¯æ‰“åŒ…çš„äº¤æ˜“
    pub async fn get_pending_transactions(&self, limit: usize) -> Vec<Transaction> {
        let pending = self.pending.read().await;
        
        let mut txs = Vec::new();
        
        for (_sender, sender_txs) in pending.iter() {
            for (_nonce, tx) in sender_txs.iter() {
                if txs.len() >= limit {
                    return txs;
                }
                txs.push(tx.clone());
            }
        }
        
        txs
    }
    
    /// éªŒè¯äº¤æ˜“
    fn validate_transaction(&self, tx: &Transaction) -> Result<(), Error> {
        // 1. éªŒè¯ç­¾å
        tx.verify_signature()?;
        
        // 2. æ£€æŸ¥Gasä»·æ ¼
        if tx.gas_price() < self.config.min_gas_price {
            return Err(Error::GasPriceTooLow);
        }
        
        // 3. å…¶ä»–éªŒè¯
        
        Ok(())
    }
    
    /// æ£€æŸ¥äº¤æ˜“æ± æ˜¯å¦å·²æ»¡
    async fn is_full(&self) -> bool {
        self.by_hash.read().await.len() >= self.config.max_transactions
    }
    
    /// é©±é€æœ€ä½ä¼˜å…ˆçº§äº¤æ˜“
    async fn evict_lowest_priority_tx(&self) -> Result<(), Error> {
        // æ‰¾åˆ°Gasä»·æ ¼æœ€ä½çš„äº¤æ˜“å¹¶ç§»é™¤
        let by_hash = self.by_hash.read().await;
        
        if let Some((hash, _)) = by_hash.iter()
            .min_by_key(|(_, tx)| tx.gas_price()) {
            let hash = hash.clone();
            drop(by_hash);
            self.remove_transaction(&hash).await?;
        }
        
        Ok(())
    }
    
    /// æ¸…ç†è¿‡æœŸäº¤æ˜“
    pub async fn cleanup_expired(&self) {
        let mut interval = tokio::time::interval(Duration::from_secs(60));
        
        loop {
            interval.tick().await;
            
            let now = SystemTime::now();
            let mut expired = Vec::new();
            
            {
                let by_hash = self.by_hash.read().await;
                for (hash, tx) in by_hash.iter() {
                    if let Ok(duration) = now.duration_since(tx.timestamp()) {
                        if duration > self.config.tx_ttl {
                            expired.push(hash.clone());
                        }
                    }
                }
            }
            
            for hash in expired {
                let _ = self.remove_transaction(&hash).await;
            }
        }
    }
}
```

### 4.2 äº¤æ˜“ç´¢å¼•

```rust
/// äº¤æ˜“ç´¢å¼•ç®¡ç†å™¨
#[derive(Debug)]
pub struct TransactionIndex {
    /// å­˜å‚¨å¼•æ“
    db: Arc<DB>,
}

impl TransactionIndex {
    /// ç´¢å¼•äº¤æ˜“
    pub async fn index_transaction(
        &self,
        tx_hash: Hash,
        block_hash: Hash,
        block_height: u64,
        tx_index: u32
    ) -> Result<(), Error> {
        let index_entry = TransactionIndexEntry {
            block_hash,
            block_height,
            tx_index,
        };
        
        let data = bincode::serialize(&index_entry)?;
        self.db.put(
            format!("tx:{}", tx_hash).as_bytes(),
            &data
        )?;
        
        Ok(())
    }
    
    /// æŸ¥è¯¢äº¤æ˜“ä½ç½®
    pub async fn get_transaction_location(
        &self,
        tx_hash: &Hash
    ) -> Result<Option<TransactionIndexEntry>, Error> {
        let key = format!("tx:{}", tx_hash);
        match self.db.get(key.as_bytes())? {
            Some(data) => {
                let entry: TransactionIndexEntry = bincode::deserialize(&data)?;
                Ok(Some(entry))
            },
            None => Ok(None),
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TransactionIndexEntry {
    pub block_hash: Hash,
    pub block_height: u64,
    pub tx_index: u32,
}
```

### 4.3 å†å²äº¤æ˜“æŸ¥è¯¢

```rust
/// äº¤æ˜“å­˜å‚¨
#[derive(Debug)]
pub struct TransactionStore {
    /// å­˜å‚¨å¼•æ“
    db: Arc<DB>,
    /// äº¤æ˜“ç´¢å¼•
    index: Arc<TransactionIndex>,
}

impl TransactionStore {
    /// å­˜å‚¨äº¤æ˜“
    pub async fn put_transaction(
        &self,
        tx: Transaction,
        block_hash: Hash,
        block_height: u64,
        tx_index: u32
    ) -> Result<(), Error> {
        let tx_hash = tx.hash();
        
        // 1. å­˜å‚¨äº¤æ˜“æ•°æ®
        let data = bincode::serialize(&tx)?;
        self.db.put(tx_hash.as_bytes(), &data)?;
        
        // 2. åˆ›å»ºç´¢å¼•
        self.index.index_transaction(
            tx_hash,
            block_hash,
            block_height,
            tx_index
        ).await?;
        
        Ok(())
    }
    
    /// è¯»å–äº¤æ˜“
    pub async fn get_transaction(&self, tx_hash: &Hash) -> Result<Option<Transaction>, Error> {
        match self.db.get(tx_hash.as_bytes())? {
            Some(data) => {
                let tx: Transaction = bincode::deserialize(&data)?;
                Ok(Some(tx))
            },
            None => Ok(None),
        }
    }
    
    /// æŸ¥è¯¢åœ°å€çš„æ‰€æœ‰äº¤æ˜“
    pub async fn get_transactions_by_address(
        &self,
        address: &Address,
        limit: usize
    ) -> Result<Vec<Transaction>, Error> {
        // ä»åœ°å€ç´¢å¼•ä¸­æŸ¥è¯¢
        let key_prefix = format!("addr:{}", address);
        let mut txs = Vec::new();
        
        let iter = self.db.prefix_iterator(key_prefix.as_bytes());
        
        for item in iter.take(limit) {
            let (_key, value) = item?;
            let tx_hash: Hash = bincode::deserialize(&value)?;
            
            if let Some(tx) = self.get_transaction(&tx_hash).await? {
                txs.push(tx);
            }
        }
        
        Ok(txs)
    }
}
```

## 5. å­˜å‚¨å¼•æ“

### 5.1 RocksDBé›†æˆ

```rust
use rocksdb::{DB, Options, WriteBatch, IteratorMode};

/// RocksDBå­˜å‚¨å¼•æ“
#[derive(Debug)]
pub struct RocksDBStorage {
    db: Arc<DB>,
    write_buffer_size: usize,
}

impl RocksDBStorage {
    /// åˆ›å»ºRocksDBå®ä¾‹
    pub fn new(path: &str) -> Result<Self, Error> {
        let mut opts = Options::default();
        opts.create_if_missing(true);
        opts.set_compression_type(rocksdb::DBCompressionType::Lz4);
        opts.set_write_buffer_size(64 * 1024 * 1024); // 64MB
        opts.set_max_write_buffer_number(3);
        opts.set_target_file_size_base(64 * 1024 * 1024);
        opts.set_level_zero_file_num_compaction_trigger(4);
        
        let db = DB::open(&opts, path)?;
        
        Ok(Self {
            db: Arc::new(db),
            write_buffer_size: 64 * 1024 * 1024,
        })
    }
    
    /// æ‰¹é‡å†™å…¥
    pub fn batch_write(&self, operations: Vec<(Vec<u8>, Vec<u8>)>) -> Result<(), Error> {
        let mut batch = WriteBatch::default();
        
        for (key, value) in operations {
            batch.put(&key, &value);
        }
        
        self.db.write(batch)?;
        Ok(())
    }
    
    /// èŒƒå›´æŸ¥è¯¢
    pub fn range_scan(&self, start: &[u8], end: &[u8]) -> Result<Vec<(Vec<u8>, Vec<u8>)>, Error> {
        let mut results = Vec::new();
        
        let iter = self.db.iterator(IteratorMode::From(start, rocksdb::Direction::Forward));
        
        for item in iter {
            let (key, value) = item?;
            if key.as_ref() >= end {
                break;
            }
            results.push((key.to_vec(), value.to_vec()));
        }
        
        Ok(results)
    }
}
```

### 5.2 LMDBé›†æˆ

```rust
use lmdb::{Environment, Database, Transaction};

/// LMDBå­˜å‚¨å¼•æ“
#[derive(Debug)]
pub struct LMDBStorage {
    env: Arc<Environment>,
    db: Database,
}

impl LMDBStorage {
    /// åˆ›å»ºLMDBå®ä¾‹
    pub fn new(path: &str, map_size: usize) -> Result<Self, Error> {
        let env = Environment::new()
            .set_map_size(map_size)
            .set_max_dbs(10)
            .open(Path::new(path))?;
        
        let db = env.open_db(None)?;
        
        Ok(Self {
            env: Arc::new(env),
            db,
        })
    }
    
    /// è¯»å–
    pub fn get(&self, key: &[u8]) -> Result<Option<Vec<u8>>, Error> {
        let txn = self.env.begin_ro_txn()?;
        
        match txn.get(self.db, &key) {
            Ok(value) => Ok(Some(value.to_vec())),
            Err(lmdb::Error::NotFound) => Ok(None),
            Err(e) => Err(e.into()),
        }
    }
    
    /// å†™å…¥
    pub fn put(&self, key: &[u8], value: &[u8]) -> Result<(), Error> {
        let mut txn = self.env.begin_rw_txn()?;
        txn.put(self.db, &key, &value, lmdb::WriteFlags::empty())?;
        txn.commit()?;
        Ok(())
    }
}
```

### 5.3 è‡ªå®šä¹‰å­˜å‚¨å¼•æ“

```rust
/// å­˜å‚¨å¼•æ“æŠ½è±¡æ¥å£
#[async_trait::async_trait]
pub trait StorageEngine: Send + Sync {
    /// è¯»å–
    async fn get(&self, key: &[u8]) -> Result<Option<Vec<u8>>, Error>;
    
    /// å†™å…¥
    async fn put(&self, key: &[u8], value: &[u8]) -> Result<(), Error>;
    
    /// åˆ é™¤
    async fn delete(&self, key: &[u8]) -> Result<(), Error>;
    
    /// æ‰¹é‡å†™å…¥
    async fn batch_write(&self, operations: Vec<WriteOperation>) -> Result<(), Error>;
    
    /// è¿­ä»£å™¨
    fn iter(&self, start: &[u8]) -> Box<dyn Iterator<Item = Result<(Vec<u8>, Vec<u8>), Error>>>;
}

#[derive(Debug, Clone)]
pub enum WriteOperation {
    Put { key: Vec<u8>, value: Vec<u8> },
    Delete { key: Vec<u8> },
}
```

## 6. ç¼“å­˜ç­–ç•¥

### 6.1 LRUç¼“å­˜

```rust
use lru::LruCache;

/// LRUç¼“å­˜å®ç°
#[derive(Debug)]
pub struct LruCacheImpl<K, V> {
    cache: Arc<RwLock<LruCache<K, V>>>,
    hits: Arc<AtomicU64>,
    misses: Arc<AtomicU64>,
}

impl<K: Hash + Eq, V: Clone> LruCacheImpl<K, V> {
    pub fn new(capacity: usize) -> Self {
        Self {
            cache: Arc::new(RwLock::new(LruCache::new(capacity))),
            hits: Arc::new(AtomicU64::new(0)),
            misses: Arc::new(AtomicU64::new(0)),
        }
    }
    
    pub async fn get(&self, key: &K) -> Option<V> {
        match self.cache.write().await.get(key) {
            Some(value) => {
                self.hits.fetch_add(1, Ordering::Relaxed);
                Some(value.clone())
            },
            None => {
                self.misses.fetch_add(1, Ordering::Relaxed);
                None
            },
        }
    }
    
    pub async fn put(&self, key: K, value: V) {
        self.cache.write().await.put(key, value);
    }
    
    pub fn hit_rate(&self) -> f64 {
        let hits = self.hits.load(Ordering::Relaxed) as f64;
        let misses = self.misses.load(Ordering::Relaxed) as f64;
        let total = hits + misses;
        
        if total == 0.0 {
            0.0
        } else {
            hits / total
        }
    }
}
```

### 6.2 å¤šçº§ç¼“å­˜

```rust
/// å¤šçº§ç¼“å­˜ç³»ç»Ÿ
#[derive(Debug)]
pub struct MultiLevelCache {
    /// L1: å†…å­˜ç¼“å­˜ï¼ˆçƒ­æ•°æ®ï¼‰
    l1_cache: Arc<LruCacheImpl<Hash, Vec<u8>>>,
    /// L2: SSDç¼“å­˜ï¼ˆæ¸©æ•°æ®ï¼‰
    l2_cache: Option<Arc<LruCacheImpl<Hash, Vec<u8>>>>,
    /// L3: æŒä¹…åŒ–å­˜å‚¨
    storage: Arc<dyn StorageEngine>,
}

impl MultiLevelCache {
    pub async fn get(&self, key: &Hash) -> Result<Option<Vec<u8>>, Error> {
        // 1. å°è¯•L1ç¼“å­˜
        if let Some(value) = self.l1_cache.get(key).await {
            return Ok(Some(value));
        }
        
        // 2. å°è¯•L2ç¼“å­˜
        if let Some(l2) = &self.l2_cache {
            if let Some(value) = l2.get(key).await {
                // æå‡åˆ°L1
                self.l1_cache.put(key.clone(), value.clone()).await;
                return Ok(Some(value));
            }
        }
        
        // 3. ä»å­˜å‚¨è¯»å–
        if let Some(value) = self.storage.get(key.as_bytes()).await? {
            // ç¼“å­˜åˆ°L2å’ŒL1
            if let Some(l2) = &self.l2_cache {
                l2.put(key.clone(), value.clone()).await;
            }
            self.l1_cache.put(key.clone(), value.clone()).await;
            return Ok(Some(value));
        }
        
        Ok(None)
    }
    
    pub async fn put(&self, key: Hash, value: Vec<u8>) -> Result<(), Error> {
        // åŒæ—¶å†™å…¥æ‰€æœ‰å±‚
        self.l1_cache.put(key.clone(), value.clone()).await;
        
        if let Some(l2) = &self.l2_cache {
            l2.put(key.clone(), value.clone()).await;
        }
        
        self.storage.put(key.as_bytes(), &value).await?;
        
        Ok(())
    }
}
```

### 6.3 é¢„è¯»ä¼˜åŒ–

```rust
/// é¢„è¯»ç®¡ç†å™¨
#[derive(Debug)]
pub struct Prefetcher {
    /// é¢„è¯»çª—å£å¤§å°
    window_size: usize,
    /// åå°é¢„è¯»ä»»åŠ¡
    prefetch_tasks: Arc<RwLock<Vec<tokio::task::JoinHandle<()>>>>,
}

impl Prefetcher {
    /// é¢„è¯»åŒºå—èŒƒå›´
    pub async fn prefetch_blocks(
        &self,
        start_height: u64,
        cache: Arc<LruCacheImpl<Hash, Block>>,
        storage: Arc<BlockStore>
    ) {
        let end_height = start_height + self.window_size as u64;
        
        let handle = tokio::spawn(async move {
            for height in start_height..end_height {
                if let Ok(Some(block)) = storage.get_block_by_height(height).await {
                    let hash = block.hash();
                    cache.put(hash, block).await;
                }
            }
        });
        
        self.prefetch_tasks.write().await.push(handle);
    }
}
```

## 7. æ•°æ®æŒä¹…åŒ–

### 7.1 WALæ—¥å¿—

```rust
/// Write-Ahead Logå®ç°
#[derive(Debug)]
pub struct WriteAheadLog {
    /// WALæ–‡ä»¶è·¯å¾„
    log_path: PathBuf,
    /// å½“å‰WALæ–‡ä»¶
    current_file: Arc<RwLock<File>>,
    /// LSNï¼ˆæ—¥å¿—åºåˆ—å·ï¼‰
    lsn: Arc<AtomicU64>,
}

impl WriteAheadLog {
    /// å†™å…¥æ—¥å¿—æ¡ç›®
    pub async fn append(&self, entry: LogEntry) -> Result<u64, Error> {
        let lsn = self.lsn.fetch_add(1, Ordering::SeqCst);
        
        let mut record = LogRecord {
            lsn,
            entry,
            checksum: 0,
        };
        
        // è®¡ç®—æ ¡éªŒå’Œ
        record.checksum = record.calculate_checksum();
        
        // åºåˆ—åŒ–
        let data = bincode::serialize(&record)?;
        
        // å†™å…¥æ–‡ä»¶
        let mut file = self.current_file.write().await;
        file.write_all(&data).await?;
        file.flush().await?;
        
        Ok(lsn)
    }
    
    /// è¯»å–æ—¥å¿—
    pub async fn read_from(&self, lsn: u64) -> Result<Vec<LogRecord>, Error> {
        let mut records = Vec::new();
        
        // è¯»å–WALæ–‡ä»¶å¹¶è§£ææ—¥å¿—è®°å½•
        // å®ç°æ—¥å¿—è¯»å–é€»è¾‘
        
        Ok(records)
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogRecord {
    pub lsn: u64,
    pub entry: LogEntry,
    pub checksum: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum LogEntry {
    Put { key: Vec<u8>, value: Vec<u8> },
    Delete { key: Vec<u8> },
    Commit,
    Abort,
}

impl LogRecord {
    fn calculate_checksum(&self) -> u32 {
        // å®ç°CRC32æ ¡éªŒå’Œ
        0
    }
}
```

### 7.2 æ£€æŸ¥ç‚¹

```rust
/// æ£€æŸ¥ç‚¹ç®¡ç†å™¨
#[derive(Debug)]
pub struct CheckpointManager {
    /// å­˜å‚¨å¼•æ“
    storage: Arc<dyn StorageEngine>,
    /// WAL
    wal: Arc<WriteAheadLog>,
    /// æ£€æŸ¥ç‚¹é—´éš”
    checkpoint_interval: Duration,
}

impl CheckpointManager {
    /// åˆ›å»ºæ£€æŸ¥ç‚¹
    pub async fn create_checkpoint(&self) -> Result<Checkpoint, Error> {
        let checkpoint_id = uuid::Uuid::new_v4();
        let lsn = self.wal.lsn.load(Ordering::SeqCst);
        
        // 1. åˆ·æ–°æ‰€æœ‰è„æ•°æ®åˆ°ç£ç›˜
        // 2. è®°å½•å½“å‰LSN
        // 3. åˆ›å»ºæ£€æŸ¥ç‚¹å…ƒæ•°æ®
        
        let checkpoint = Checkpoint {
            id: checkpoint_id,
            lsn,
            timestamp: SystemTime::now(),
        };
        
        Ok(checkpoint)
    }
    
    /// å®šæœŸåˆ›å»ºæ£€æŸ¥ç‚¹
    pub async fn periodic_checkpoint(&self) {
        let mut interval = tokio::time::interval(self.checkpoint_interval);
        
        loop {
            interval.tick().await;
            
            if let Err(e) = self.create_checkpoint().await {
                eprintln!("æ£€æŸ¥ç‚¹åˆ›å»ºå¤±è´¥: {:?}", e);
            }
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Checkpoint {
    pub id: uuid::Uuid,
    pub lsn: u64,
    pub timestamp: SystemTime,
}
```

### 7.3 å´©æºƒæ¢å¤

```rust
/// å´©æºƒæ¢å¤ç®¡ç†å™¨
#[derive(Debug)]
pub struct CrashRecovery {
    wal: Arc<WriteAheadLog>,
    storage: Arc<dyn StorageEngine>,
}

impl CrashRecovery {
    /// æ‰§è¡Œå´©æºƒæ¢å¤
    pub async fn recover(&self) -> Result<RecoveryStats, Error> {
        // 1. æ‰¾åˆ°æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹
        let checkpoint = self.find_latest_checkpoint().await?;
        
        // 2. ä»æ£€æŸ¥ç‚¹ä¹‹åçš„WALé‡æ”¾æ—¥å¿—
        let records = self.wal.read_from(checkpoint.lsn).await?;
        
        let mut stats = RecoveryStats::default();
        
        for record in records {
            match record.entry {
                LogEntry::Put { key, value } => {
                    self.storage.put(&key, &value).await?;
                    stats.operations_replayed += 1;
                },
                LogEntry::Delete { key } => {
                    self.storage.delete(&key).await?;
                    stats.operations_replayed += 1;
                },
                LogEntry::Commit => {
                    stats.transactions_committed += 1;
                },
                LogEntry::Abort => {
                    stats.transactions_aborted += 1;
                },
            }
        }
        
        Ok(stats)
    }
    
    async fn find_latest_checkpoint(&self) -> Result<Checkpoint, Error> {
        // æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
        todo!()
    }
}

#[derive(Debug, Default)]
pub struct RecoveryStats {
    pub operations_replayed: usize,
    pub transactions_committed: usize,
    pub transactions_aborted: usize,
}
```

## 8. å­˜å‚¨ä¼˜åŒ–

### 8.1 å‹ç¼©ç­–ç•¥

```rust
use flate2::Compression;
use flate2::write::{GzEncoder, GzDecoder};

/// æ•°æ®å‹ç¼©å™¨
#[derive(Debug)]
pub struct DataCompressor {
    compression_level: Compression,
}

impl DataCompressor {
    /// å‹ç¼©æ•°æ®
    pub fn compress(&self, data: &[u8]) -> Result<Vec<u8>, Error> {
        let mut encoder = GzEncoder::new(Vec::new(), self.compression_level);
        encoder.write_all(data)?;
        let compressed = encoder.finish()?;
        Ok(compressed)
    }
    
    /// è§£å‹æ•°æ®
    pub fn decompress(&self, data: &[u8]) -> Result<Vec<u8>, Error> {
        let mut decoder = GzDecoder::new(Vec::new());
        decoder.write_all(data)?;
        let decompressed = decoder.finish()?;
        Ok(decompressed)
    }
}
```

### 8.2 æ•°æ®åˆ†ç‰‡

```rust
/// æ•°æ®åˆ†ç‰‡ç®¡ç†å™¨
#[derive(Debug)]
pub struct DataSharding {
    /// åˆ†ç‰‡æ•°é‡
    shard_count: usize,
    /// åˆ†ç‰‡å­˜å‚¨
    shards: Vec<Arc<dyn StorageEngine>>,
}

impl DataSharding {
    /// è®¡ç®—åˆ†ç‰‡ID
    fn shard_id(&self, key: &[u8]) -> usize {
        use std::hash::{Hash, Hasher};
        use std::collections::hash_map::DefaultHasher;
        
        let mut hasher = DefaultHasher::new();
        key.hash(&mut hasher);
        (hasher.finish() as usize) % self.shard_count
    }
    
    /// è¯»å–æ•°æ®
    pub async fn get(&self, key: &[u8]) -> Result<Option<Vec<u8>>, Error> {
        let shard_id = self.shard_id(key);
        self.shards[shard_id].get(key).await
    }
    
    /// å†™å…¥æ•°æ®
    pub async fn put(&self, key: &[u8], value: &[u8]) -> Result<(), Error> {
        let shard_id = self.shard_id(key);
        self.shards[shard_id].put(key, value).await
    }
}
```

### 8.3 å½’æ¡£ä¸å‰ªæ

```rust
/// å½’æ¡£ç®¡ç†å™¨
#[derive(Debug)]
pub struct ArchiveManager {
    /// æ´»è·ƒå­˜å‚¨
    active_storage: Arc<dyn StorageEngine>,
    /// å½’æ¡£å­˜å‚¨
    archive_storage: Arc<dyn StorageEngine>,
    /// å½’æ¡£é˜ˆå€¼ï¼ˆåŒºå—å¹´é¾„ï¼‰
    archive_threshold: Duration,
}

impl ArchiveManager {
    /// å½’æ¡£æ—§æ•°æ®
    pub async fn archive_old_data(&self) -> Result<ArchiveStats, Error> {
        let mut stats = ArchiveStats::default();
        let cutoff_time = SystemTime::now() - self.archive_threshold;
        
        // æŸ¥æ‰¾éœ€è¦å½’æ¡£çš„æ•°æ®
        // ç§»åŠ¨åˆ°å½’æ¡£å­˜å‚¨
        // ä»æ´»è·ƒå­˜å‚¨ä¸­åˆ é™¤
        
        Ok(stats)
    }
}

#[derive(Debug, Default)]
pub struct ArchiveStats {
    pub blocks_archived: u64,
    pub data_size_archived: u64,
}
```

## 9. æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†åŒºå—é“¾å­˜å‚¨ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°ï¼ŒåŒ…æ‹¬ï¼š

1. **å­˜å‚¨æ¶æ„**ï¼šå¤šå±‚å­˜å‚¨ç»“æ„ã€å­˜å‚¨ç±»å‹ã€æ•°æ®ç»„ç»‡
2. **åŒºå—å­˜å‚¨**ï¼šæ•°æ®ç»“æ„ã€ç´¢å¼•ã€æ£€ç´¢
3. **çŠ¶æ€å­˜å‚¨**ï¼šMerkle Patricia Trieã€å¿«ç…§ã€å‰ªæ
4. **äº¤æ˜“å­˜å‚¨**ï¼šäº¤æ˜“æ± ã€ç´¢å¼•ã€å†å²æŸ¥è¯¢
5. **å­˜å‚¨å¼•æ“**ï¼šRocksDBã€LMDBã€è‡ªå®šä¹‰å¼•æ“
6. **ç¼“å­˜ç­–ç•¥**ï¼šLRUç¼“å­˜ã€å¤šçº§ç¼“å­˜ã€é¢„è¯»ä¼˜åŒ–
7. **æ•°æ®æŒä¹…åŒ–**ï¼šWALã€æ£€æŸ¥ç‚¹ã€å´©æºƒæ¢å¤
8. **å­˜å‚¨ä¼˜åŒ–**ï¼šå‹ç¼©ã€åˆ†ç‰‡ã€å½’æ¡£

è¿™äº›å®ç°ä¸ºæ„å»ºé«˜æ•ˆã€å¯é ã€å¯æ‰©å±•çš„åŒºå—é“¾å­˜å‚¨ç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æŠ€æœ¯æ–¹æ¡ˆã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025å¹´10æœˆ17æ—¥  
**ä½œè€…**: RuståŒºå—é“¾æŠ€æœ¯å›¢é˜Ÿ  
**ç›¸å…³æ–‡æ¡£**:
- [08_ARCHITECTURE_DESIGN.md](./08_ARCHITECTURE_DESIGN.md) - ç³»ç»Ÿæ¶æ„è®¾è®¡
- [09_NETWORK_PROTOCOLS.md](./09_NETWORK_PROTOCOLS.md) - ç½‘ç»œåè®®è®¾è®¡
- [11_PERFORMANCE_OPTIMIZATION.md](./11_PERFORMANCE_OPTIMIZATION.md) - æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

